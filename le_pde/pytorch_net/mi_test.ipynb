{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Greg Ver Steeg\n",
    "# http://www.isi.edu/~gregv/npeet.html\n",
    "\n",
    "import mi as ee\n",
    "from math import log, pi\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import random\n",
    "from numpy.linalg import det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differential entropy estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For a uniform distribution with width alpha, the differential entropy is log_2 alpha, setting alpha = 2\")\n",
    "print(\"and using k=1, 2, 3, 4, 5\")\n",
    "print(\"result:\", [ee.entropy([[2 * random.random()] for i in range(1000)], k=j + 1) for j in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntry = [10, 25, 50, 100, 200]  # , 1000, 2000] #Number of samples to use in estimate\n",
    "nsamples = 100  # Number of times to est mutual information for CI\n",
    "samplo = int(0.025 * nsamples)  # confidence intervals\n",
    "samphi = int(0.975 * nsamples)\n",
    "\n",
    "print('\\nGaussian random variables\\n')\n",
    "print('Conditional Mutual Information')\n",
    "d1 = [1, 1, 0]\n",
    "d2 = [1, 0, 1]\n",
    "d3 = [0, 1, 1]\n",
    "mat = [d1, d2, d3]\n",
    "tmat = np.transpose(mat)\n",
    "diag = [[3, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "mean = np.array([0, 0, 0])\n",
    "cov = np.dot(tmat, np.dot(diag, mat))\n",
    "print('covariance matrix')\n",
    "print(cov)\n",
    "trueent = -0.5 * (3 + log(8. * pi * pi * pi * det(cov)))\n",
    "trueent += -0.5 * (1 + log(2. * pi * cov[2][2]))  # z sub\n",
    "trueent += 0.5 * (2 + log(4. * pi * pi * det([[cov[0][0], cov[0][2]], [cov[2][0], cov[2][2]]])))  # xz sub\n",
    "trueent += 0.5 * (2 + log(4. * pi * pi * det([[cov[1][1], cov[1][2]], [cov[2][1], cov[2][2]]])))  # yz sub\n",
    "print('true CMI(x:y|x)', trueent / log(2))\n",
    "\n",
    "ent = []\n",
    "err = []\n",
    "for NN in Ntry:\n",
    "    tempent = []\n",
    "    for j in range(nsamples):\n",
    "        points = nr.multivariate_normal(mean, cov, NN)\n",
    "        x = [point[:1] for point in points]\n",
    "        y = [point[1:2] for point in points]\n",
    "        z = [point[2:] for point in points]\n",
    "        tempent.append(ee.cmi(x, y, z))\n",
    "    tempent.sort()\n",
    "    tempmean = np.mean(tempent)\n",
    "    ent.append(tempmean)\n",
    "    err.append((tempmean - tempent[samplo], tempent[samphi] - tempmean))\n",
    "\n",
    "print('samples used', Ntry)\n",
    "print('estimated CMI', ent)\n",
    "print('95% conf int. (a, b) means (mean - a, mean + b)is interval\\n', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mutual Information')\n",
    "trueent = 0.5 * (1 + log(2. * pi * cov[0][0]))  # x sub\n",
    "trueent += 0.5 * (1 + log(2. * pi * cov[1][1]))  # y sub\n",
    "trueent += -0.5 * (2 + log(4. * pi * pi * det([[cov[0][0], cov[0][1]], [cov[1][0], cov[1][1]]])))  # xz sub\n",
    "print('true MI(x:y)', trueent / log(2))\n",
    "\n",
    "ent = []\n",
    "err = []\n",
    "for NN in Ntry:\n",
    "    tempent = []\n",
    "    for j in range(nsamples):\n",
    "        points = nr.multivariate_normal(mean, cov, NN)\n",
    "        x = [point[:1] for point in points]\n",
    "        y = [point[1:2] for point in points]\n",
    "        tempent.append(ee.mi(x, y))\n",
    "    tempent.sort()\n",
    "    tempmean = np.mean(tempent)\n",
    "    ent.append(tempmean)\n",
    "    err.append((tempmean - tempent[samplo], tempent[samphi] - tempmean))\n",
    "\n",
    "print('samples used', Ntry)\n",
    "print('estimated MI', ent)\n",
    "print('95% conf int.\\n', err)\n",
    "\n",
    "\n",
    "print('\\nIF you permute the indices of x, e.g., MI(X:Y) = 0')\n",
    "# You can use shuffle_test method to just get mean, standard deviation\n",
    "ent = []\n",
    "err = []\n",
    "for NN in Ntry:\n",
    "    tempent = []\n",
    "    for j in range(nsamples):\n",
    "        points = nr.multivariate_normal(mean, cov, NN)\n",
    "        x = [point[:1] for point in points]\n",
    "        y = [point[1:2] for point in points]\n",
    "        random.shuffle(y)\n",
    "        tempent.append(ee.mi(x, y))\n",
    "    tempent.sort()\n",
    "    tempmean = np.mean(tempent)\n",
    "    ent.append(tempmean)\n",
    "    err.append((tempmean - tempent[samplo], tempent[samphi] - tempmean))\n",
    "\n",
    "print('samples used', Ntry)\n",
    "print('estimated MI', ent)\n",
    "print('95% conf int.\\n', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTest of the discrete entropy estimators\\n\")\n",
    "print(\"For z = y xor x, w/x, y uniform random binary, we should get H(x)=H(y)=H(z) = 1, H(x:y) etc = 0, H(x:y|z) = 1\")\n",
    "x = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "y = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "z = [0, 1, 0, 1, 1, 0, 1, 0]\n",
    "print(\"H(x), H(y), H(z)\", ee.entropyd(x), ee.entropyd(y), ee.entropyd(z))\n",
    "print(\"H(x:y), etc\", ee.midd(x, y), ee.midd(z, y), ee.midd(x, z))\n",
    "print(\"H(x:y|z), etc\", ee.cmidd(x, y, z), ee.cmidd(z, y, x), ee.cmidd(x, z, y))\n",
    "\n",
    "def test_discrete(size=1000, y_func=lambda x: x**2):\n",
    "    print(\"\\nTest discrete.\")\n",
    "    from collections import defaultdict\n",
    "    information = defaultdict(list)\n",
    "    y_entropy = defaultdict(list)\n",
    "    x_entropy = []\n",
    "    for trial in range(10):\n",
    "        x = np.random.randint(low=0, high=10, size=size)\n",
    "\n",
    "        y_random = np.random.randint(low=53, high=53 + 5, size=size)\n",
    "        y_deterministic = y_func(x)\n",
    "        noise = np.random.randint(low=0, high=10, size=size)\n",
    "        y_noisy = y_deterministic + noise\n",
    "\n",
    "        information['random'].append(ee.midd(x, y_random))\n",
    "        information['deterministic'].append(ee.midd(x, y_deterministic))\n",
    "        information['noisy'].append(ee.midd(x, y_noisy))\n",
    "\n",
    "        x_entropy.append(ee.entropyd(x))\n",
    "        y_entropy['random'].append(ee.entropyd(y_random))\n",
    "        y_entropy['deterministic'].append(ee.entropyd(y_deterministic))\n",
    "        y_entropy['noisy'].append(ee.entropyd(y_noisy))\n",
    "    x_entropy = np.mean(x_entropy)\n",
    "    for experiment_name in information.keys():\n",
    "        max_information = min(x_entropy, np.mean(y_entropy[experiment_name]))\n",
    "        print(f\"{experiment_name}: I(X; Y) = {np.mean(information[experiment_name]):.4f} \"\n",
    "              f\"Â± {np.std(information[experiment_name]):.4f} (maximum possible {max_information:.4f})\")\n",
    "\n",
    "\n",
    "test_discrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Div estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nKl divergence estimator (not symmetric, not required to have same num samples in each sample set\")\n",
    "print(\"should be 0 for same distribution\")\n",
    "sample1 = np.random.rand(200,2)\n",
    "sample2 = np.random.rand(300,2)\n",
    "print('result:', ee.kldiv(sample1, sample2))\n",
    "print(\"should be infinite for totally disjoint distributions (but this estimator has an upper bound like log(dist) between disjoint prob. masses)\")\n",
    "sample3 = np.random.rand(200,2) + 3\n",
    "print('result:', ee.kldiv(sample1, sample3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

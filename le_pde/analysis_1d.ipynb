{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41481747-0049-4511-b512-df8489041673",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "import gc\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "from numbers import Number\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 1500\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.width = 1000\n",
    "pd.set_option('max_colwidth', 400)\n",
    "import pdb\n",
    "import pickle\n",
    "import pprint as pp\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from deepsnap.batch import Batch as deepsnap_Batch\n",
    "import xarray as xr\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "from le_pde.argparser import arg_parse\n",
    "from le_pde.datasets.load_dataset import load_data\n",
    "from le_pde.models import load_model\n",
    "from le_pde.pytorch_net.util import groupby_add_keys, filter_df, get_unique_keys_df, Attr_Dict, Printer, get_num_params, get_machine_name, pload, pdump, to_np_array, get_pdict, reshape_weight_to_matrix, ddeepcopy as deepcopy, plot_vectors, record_data, filter_filename, Early_Stopping, str2bool, get_filename_short, print_banner, plot_matrices, get_num_params, init_args, filter_kwargs, to_string, COLOR_LIST\n",
    "from le_pde.utils import update_legacy_default_hyperparam, EXP_PATH\n",
    "from le_pde.utils import deepsnap_to_pyg, LpLoss, to_cpu, to_tuple_shape, parse_multi_step, loss_op, get_device\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "p = Printer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a797ba8-d78b-47a5-bcb6-00e91b487da1",
   "metadata": {},
   "source": [
    "## 0. Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67193e7b-5777-4aac-9426-f91e6837d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting:\n",
    "def plot_learning_curve(data_record):\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(data_record[\"epoch\"], data_record[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"val_loss\"], label=\"val\")\n",
    "    plt.plot(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"test_loss\"], label=\"test\")\n",
    "    plt.title(\"Learning curve, linear scale\")\n",
    "    plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.semilogy(data_record[\"epoch\"], data_record[\"train_loss\"], label=\"train\")\n",
    "    plt.semilogy(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"val_loss\"], label=\"val\")\n",
    "    plt.semilogy(data_record[\"test_epoch\"] if \"test_epoch\" in data_record else data_record[\"epoch\"], data_record[\"test_loss\"], label=\"test\")\n",
    "    plt.title(\"Learning curve, log scale\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_colorbar(matrix, vmax=None, vmin=None, cmap=\"seismic\", label=None):\n",
    "    if vmax==None:\n",
    "        vmax = matrix.max()\n",
    "        vmin = matrix.min()\n",
    "    im = plt.imshow(matrix,vmax=vmax,vmin=vmin,cmap=cmap)\n",
    "    plt.title(label)\n",
    "    im_ratio = matrix.shape[0]/matrix.shape[1]\n",
    "    plt.colorbar(im,fraction=0.046*im_ratio,pad=0.04)\n",
    "\n",
    "\n",
    "def visualize(pred, gt, animate=False):\n",
    "    if torch.is_tensor(gt):\n",
    "        gt = to_np_array(gt)\n",
    "        pred = to_np_array(pred)\n",
    "    mse_over_t = ((gt-pred)**2).mean(axis=0).mean(axis=-1)\n",
    "     \n",
    "    if not animate:\n",
    "        vmax = gt.max()\n",
    "        vmin = gt.min()\n",
    "        plt.figure(figsize=[15,5])\n",
    "        plt.subplot(1,4,1)\n",
    "        plot_colorbar(gt[:,:,0].T,label=\"gt\")\n",
    "        plt.subplot(1,4,2)\n",
    "        plot_colorbar(pred[:,:,0].T,label=\"pred\")\n",
    "        plt.subplot(1,4,3)\n",
    "        plot_colorbar((pred-gt)[:,:,0].T,vmax=np.abs(pred-gt).max(),vmin=(-1*np.abs(pred-gt).max()),label=\"diff\")\n",
    "        plt.subplot(1,4,4)\n",
    "        plt.plot(mse_over_t);plt.title(\"mse over t\");plt.yscale('log');\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def visualize_paper(pred, gt, is_save=False):\n",
    "    idx = 6\n",
    "    nx = pred.shape[0]\n",
    "\n",
    "    fontsize = 14\n",
    "    idx_list = np.arange(0, 200, 15)\n",
    "    color_list = np.linspace(0.01, 0.9, len(idx_list))\n",
    "    x_axis = np.linspace(0,16,nx)\n",
    "    cmap = matplotlib.cm.get_cmap('jet')\n",
    "\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        pred_i = to_np_array(pred[...,idx,:].squeeze())\n",
    "        rgb = cmap(color_list[i])[:3]\n",
    "        plt.plot(x_axis, pred_i, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "    plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"x\", fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "    # plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "    plt.xticks([0,8,16], [0,8,16])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.title(\"Prediction\")\n",
    "    if is_save:\n",
    "        plt.savefig(f\"1D_E2-{nx}.pdf\", bbox_inches='tight')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        y_i = to_np_array(gt[...,idx,:])\n",
    "        rgb = cmap(color_list[i])[:3]\n",
    "        plt.plot(x_axis, y_i, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "    plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "    plt.xlabel(\"x\", fontsize=fontsize)\n",
    "    plt.tick_params(labelsize=fontsize)\n",
    "    plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "    plt.xticks([0,8,16], [0,8,16])\n",
    "    plt.ylim([-2.5,2.5])\n",
    "    plt.title(\"Ground-truth\")\n",
    "    if is_save:\n",
    "        plt.savefig(f\"1D_gt-{nx}.pdf\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43519c3-b5d7-4cc6-a6b3-b75a620e13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis:\n",
    "def get_results_1d(\n",
    "    all_hash,\n",
    "    mode=\"best\",\n",
    "    exclude_idx=(None,),\n",
    "    n_rollout_steps=-1,\n",
    "    dirname=None,\n",
    "    suffix=\"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform analysis on the 1D Burgers' benchmark.\n",
    "\n",
    "    Args:\n",
    "        all_hash: a list of hashes which indicates the experiments to load for analysis\n",
    "        mode: choose from \"best\" (load the best model with lowest validation loss) or an integer, \n",
    "            e.g. -1 (last saved model), -2 (second last saved model)\n",
    "        dirname: if not None, will use the dirnaem provided. E.g. user\n",
    "        suffix: suffix for saving the analysis result.\n",
    "    \"\"\"\n",
    "\n",
    "    isplot = True\n",
    "    df_dict_list = []\n",
    "    dirname_start = dirname\n",
    "    for hash_str in all_hash:\n",
    "        df_dict = {}\n",
    "        df_dict[\"hash\"] = hash_str\n",
    "        # Load model:\n",
    "        is_found = False\n",
    "        for dirname_core in [\n",
    "             dirname_start,\n",
    "            ]:\n",
    "            filename = filter_filename(EXP_PATH + dirname_core, include=hash_str)\n",
    "            if len(filename) == 1:\n",
    "                is_found = True\n",
    "                break\n",
    "        if not is_found:\n",
    "            print(f\"hash {hash_str} does not exist in {dirname}! Please pass in the correct dirname.\")\n",
    "            continue\n",
    "        dirname = EXP_PATH + dirname_core\n",
    "        if not dirname.endswith(\"/\"):\n",
    "            dirname += \"/\"\n",
    "\n",
    "        try:\n",
    "            data_record = pload(dirname + filename[0])\n",
    "        except Exception as e:\n",
    "            # p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=100)\n",
    "            print(f\"error {e} in hash_str {hash_str}\")\n",
    "            continue\n",
    "        p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=160)\n",
    "        if isplot:\n",
    "            plot_learning_curve(data_record)\n",
    "        args = init_args(update_legacy_default_hyperparam(data_record[\"args\"]))\n",
    "        args.filename = filename\n",
    "        if mode == \"best\":\n",
    "            model = load_model(data_record[\"best_model_dict\"], device=device)\n",
    "            print(\"Load the model with best validation loss.\")\n",
    "        else:\n",
    "            assert isinstance(mode, int)\n",
    "            print(f'Load the model at epoch {data_record[\"epoch\"][mode]}')\n",
    "            model = load_model(data_record[\"model_dict\"][mode], device=device)\n",
    "        model.eval()\n",
    "        # pp.pprint(args.__dict__)\n",
    "        kwargs = {}\n",
    "        if data_record[\"best_model_dict\"][\"type\"].startswith(\"GNNPolicy\"):\n",
    "            kwargs[\"is_deepsnap\"] = True\n",
    "\n",
    "        # Load test dataset:\n",
    "        args_test = deepcopy(args)\n",
    "        multi_step = (250 - 50) // args_test.temporal_bundle_steps\n",
    "        args_test.multi_step = f\"1^{multi_step}\"\n",
    "        args_test.is_test_only = True\n",
    "        n_test_traj = 128\n",
    "        (dataset_train_val, dataset_test), (train_loader, val_loader, test_loader) = load_data(args_test)\n",
    "        nx = int(args.dataset.split(\"-\")[2])\n",
    "        time_stamps_effective = len(dataset_test) // n_test_traj\n",
    "        for exclude_idx_ele in exclude_idx:\n",
    "            loss_list = []\n",
    "            pred_list = []\n",
    "            y_list = []\n",
    "            for i in range(n_test_traj):\n",
    "                idx = i * time_stamps_effective + args_test.temporal_bundle_steps\n",
    "                data = deepcopy(dataset_test[idx])\n",
    "                data = data.to(device)\n",
    "                preds, info = model(\n",
    "                    data,\n",
    "                    pred_steps=np.arange(1,n_rollout_steps+1) if n_rollout_steps != -1 else np.arange(1, max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "                    latent_pred_steps=None,\n",
    "                    is_recons=False,\n",
    "                    use_grads=False,\n",
    "                    use_pos=args.use_pos,\n",
    "                    is_y_diff=False,\n",
    "                    is_rollout=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "                y = data.node_label[\"n0\"]\n",
    "                if n_rollout_steps != -1:\n",
    "                    y = y[:,:25*n_rollout_steps]\n",
    "                pred = preds[\"n0\"].reshape(y.shape)\n",
    "                pred_list.append(pred.detach())\n",
    "                y_list.append(y.detach())\n",
    "                loss_ele = nn.MSELoss(reduction=\"sum\")(pred, y) / nx\n",
    "                loss_list.append(loss_ele.item())\n",
    "\n",
    "            loss_mean = np.mean(loss_list)\n",
    "            pred_list = torch.stack(pred_list).squeeze(-1)\n",
    "            y_list = torch.stack(y_list).squeeze(-1)\n",
    "            df_dict[f\"loss_cumu_{exclude_idx_ele}\"] = loss_mean \n",
    "            print(\"\\nTest for {} for exclude_idx={} is: {:.9f} at epoch {}, for {}/{} epochs\".format(hash_str, exclude_idx_ele, loss_mean, data_record['best_epoch'], len(data_record[\"train_loss\"]), args.epochs))\n",
    "\n",
    "            mse_full = nn.MSELoss(reduction=\"none\")(pred_list, y_list)\n",
    "            mse_time = to_np_array(mse_full.mean((0,1)))\n",
    "            p.print(\"Learning curve:\", is_datetime=False, banner_size=100)\n",
    "            plt.figure(figsize=(12,5))\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(mse_time)\n",
    "            plt.xlabel(\"rollout step\")\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.title(\"MSE vs. rollout step (linear scale)\")\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.semilogy(mse_time)\n",
    "            plt.xlabel(\"rollout step\")\n",
    "            plt.ylabel(\"MSE\")\n",
    "            plt.title(\"MSE vs. rollout step (log scale)\")\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(6,5))\n",
    "            plt.plot(mse_time.cumsum())\n",
    "            plt.title(\"cumulative MSE vs. rollout step\")\n",
    "            plt.xlabel(\"rollout step\")\n",
    "            plt.ylabel(\"cumulative MSE\")\n",
    "            plt.show()\n",
    "\n",
    "            # Visualization:\n",
    "            for idx in range(6,8):\n",
    "                p.print(f\"Example {idx*128}:\", banner_size=100, is_datetime=False)\n",
    "                data = deepcopy(dataset_test[idx*128]).to(device)\n",
    "                preds, info = model(\n",
    "                    data,\n",
    "                    pred_steps=np.arange(1,max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "                    latent_pred_steps=None,\n",
    "                    is_recons=False,\n",
    "                    use_grads=False,\n",
    "                    use_pos=args.use_pos,\n",
    "                    is_y_diff=False,\n",
    "                    is_rollout=False,\n",
    "                    **kwargs\n",
    "                )\n",
    "                y = data.node_label[\"n0\"]\n",
    "                pred = preds[\"n0\"].reshape(y.shape)\n",
    "                visualize(pred, y)\n",
    "                visualize_paper(pred, y)\n",
    "\n",
    "            p.print(f\"Individual prediction at rollout step {y.shape[1]}:\", banner_size=100, is_datetime=False)\n",
    "            time_step = -1\n",
    "            for idx in range(0, 20, 5):\n",
    "                plt.figure(figsize=(6,4))\n",
    "                plt.plot(to_np_array(pred_list[idx,:,time_step]), label=\"pred\")\n",
    "                plt.plot(to_np_array(y_list[idx,:,time_step]), \"--\", label=\"y\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "        df_dict[\"best_epoch\"] = data_record['best_epoch']\n",
    "        df_dict[\"epoch\"] = len(data_record[\"train_loss\"])\n",
    "        df_dict.update(args.__dict__)\n",
    "        df_dict_list.append(df_dict)\n",
    "    df = pd.DataFrame(df_dict_list)\n",
    "    pdump(df, f\"df_1d{suffix}.p\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27bb23-992f-4c94-9ebf-fc5bc82527a1",
   "metadata": {},
   "source": [
    "## 1. Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52d49b-4c66-4696-858f-eb5d640fb4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_hash is a list of hashes, each of which corresponds to one experiment.\n",
    "# For example, if one experiment is saved under ./results/user4/mppde1d-E2-50_train_-1_algo_contrast_ebm_False_ebmt_cd_enc_cnn-s_evo_cnn_act_elu_hid_128_lo_rmse_recef_1.0_conef_1.0_nconv_4_nlat_1_clat_3_lf_True_reg_None_id_0_Hash_Un6ae7ja_turing2.p\n",
    "# Then, the \"Un6ae7ja_turing2\" (located at the end of the filename) is the {hash}_{machine-name} of this file.\n",
    "# The \"user4\" is the \"{--exp_id}_{--date_time}\" of the training command.\n",
    "# all_hash can contain multiple hashes, and analyze them sequentially.\n",
    "all_hash = [\n",
    "    \"Un6ae7ja_turing2\",\n",
    "]\n",
    "get_results_1d(all_hash, dirname=\"user4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787fd02-89a8-4aa2-a5ce-22f8b9988f9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4a999-24a7-41b9-893b-4bf7e2afb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_str = \"Un6ae7ja_turing2\" # E2-50\n",
    "# hash_str = \"nIa6UCdr_turing2\" # E2-40\n",
    "hash_str = \"tdu+jfKw\"  # E2-100\n",
    "dirname = EXP_PATH + \"user4/\"\n",
    "all_dict = {}\n",
    "\n",
    "# Load model:\n",
    "filename = filter_filename(dirname, include=hash_str)\n",
    "if len(filename) == 0:\n",
    "    print(f\"hash {hash_str} does not exist!\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    data_record = pload(dirname + filename[0])\n",
    "except Exception as e:\n",
    "    # p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=100)\n",
    "    print(f\"error {e} in hash_str {hash_str}\")\n",
    "    raise\n",
    "p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=100)\n",
    "if isplot:\n",
    "    plot_learning_curve(data_record)\n",
    "args = init_args(update_legacy_default_hyperparam(data_record[\"args\"]))\n",
    "args.filename = filename\n",
    "args.is_test_only = True\n",
    "model = load_model(data_record[\"best_model_dict\"], device=device)\n",
    "# model = load_model(data_record[\"model_dict\"][-1], device=device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Load test dataset:\n",
    "args_test = deepcopy(args)\n",
    "multi_step = (250 - 50) // args_test.temporal_bundle_steps\n",
    "args_test.multi_step = f\"1^{multi_step}\"\n",
    "(dataset_train_val, dataset_test), (train_loader, val_loader, test_loader) = load_data(args_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0effd9-c3cd-4259-bad3-f968a7af8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 6\n",
    "nx = 100\n",
    "data = dataset_test[idx*128]\n",
    "data.to(device)\n",
    "preds, info = model(\n",
    "    data,\n",
    "    pred_steps=np.arange(1,max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "    latent_pred_steps=None,\n",
    "    is_recons=False,\n",
    "    use_grads=False,\n",
    "    is_y_diff=False,\n",
    "    is_rollout=False,\n",
    ")\n",
    "\n",
    "fontsize = 14\n",
    "idx_list = np.arange(0, 200, 15)\n",
    "color_list = np.linspace(0.01, 0.9, len(idx_list))\n",
    "x_axis = np.linspace(0,16,nx)\n",
    "cmap = matplotlib.cm.get_cmap('jet')\n",
    "\n",
    "for i, idx in enumerate(idx_list):\n",
    "    pred = to_np_array(preds[\"n0\"][...,idx,:].squeeze())\n",
    "    # y = to_np_array(data.node_label[\"n0\"][...,idx,:])\n",
    "    rgb = cmap(color_list[i])[:3]\n",
    "    plt.plot(x_axis, pred, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "plt.xlabel(\"x\", fontsize=fontsize)\n",
    "plt.tick_params(labelsize=fontsize)\n",
    "plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "plt.xticks([0,8,16], [0,8,16])\n",
    "plt.ylim([-2.2,2.])\n",
    "plt.savefig(f\"1D_E2-{nx}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"gt:\")\n",
    "for i, idx in enumerate(idx_list):\n",
    "    pred = to_np_array(preds[\"n0\"][...,idx,:].squeeze())\n",
    "    y = to_np_array(data.node_label[\"n0\"][...,idx,:])\n",
    "    rgb = cmap(color_list[i])[:3]\n",
    "    plt.plot(x_axis, y, color=rgb, label=f\"t={np.round(i*0.3, 1)}s\")\n",
    "plt.ylabel(\"u(t,x)\", fontsize=fontsize)\n",
    "plt.xlabel(\"x\", fontsize=fontsize)\n",
    "plt.tick_params(labelsize=fontsize)\n",
    "plt.legend(fontsize=10, bbox_to_anchor=[1,1])\n",
    "plt.xticks([0,8,16], [0,8,16])\n",
    "plt.ylim([-2.2,2.])\n",
    "plt.savefig(f\"1D_gt-{nx}.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea56c-8882-4479-b6cc-eb1f19c1e378",
   "metadata": {},
   "source": [
    "## 3. Timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f4d19-81f3-455a-935e-c89e5a2b1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timing_1d(all_hash, suffix=\"\"):\n",
    "    isplot = True\n",
    "\n",
    "    dirname = EXP_PATH + \"user4/\"\n",
    "    all_dict = {}\n",
    "    hash_str = all_hash[0]\n",
    "\n",
    "    # Load model:\n",
    "    filename = filter_filename(dirname, include=hash_str)\n",
    "    if len(filename) == 0:\n",
    "        dirname = EXP_PATH + \"user7/\"\n",
    "        filename = filter_filename(dirname, hash_str)\n",
    "        if len(filename) == 0:\n",
    "            print(f\"hash {hash_str} does not exist!\")\n",
    "            raise\n",
    "\n",
    "    try:\n",
    "        data_record = pload(dirname + filename[0])\n",
    "    except Exception as e:\n",
    "        # p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=100)\n",
    "        print(f\"error {e} in hash_str {hash_str}\")\n",
    "        raise\n",
    "    p.print(f\"Hash {hash_str}, best model at epoch {data_record['best_epoch']}:\", banner_size=100)\n",
    "    if isplot:\n",
    "        plot_learning_curve(data_record)\n",
    "    args = init_args(update_legacy_default_hyperparam(data_record[\"args\"]))\n",
    "    args.filename = filename\n",
    "    args.is_test_only = True\n",
    "    model = load_model(data_record[\"best_model_dict\"], device=device)\n",
    "    # model = load_model(data_record[\"model_dict\"][-1], device=device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load test dataset:\n",
    "    args_test = deepcopy(args)\n",
    "    multi_step = (250 - 50) // args_test.temporal_bundle_steps\n",
    "    args_test.multi_step = f\"1^{multi_step}\"\n",
    "\n",
    "    args_test = deepcopy(args)\n",
    "    args_test.multi_step = \"1^8\"\n",
    "    (dataset_train_val, dataset_test), (train_loader, val_loader, test_loader) = load_data(args_test)\n",
    "    idx_list = [i * 26 + 25 for i in range(128)]\n",
    "    dataset_test_selected = dataset_test[idx_list]\n",
    "    test_loader = DataLoader(dataset_test_selected, num_workers=0, collate_fn=deepsnap_Batch.collate(),\n",
    "                             batch_size=len(dataset_test_selected), shuffle=False, drop_last=False)\n",
    "    for data in test_loader:\n",
    "        break\n",
    "    data.to(device)\n",
    "\n",
    "    for hash_str in all_hash:\n",
    "        all_list_dict = {}\n",
    "    for hash_str in all_hash:\n",
    "        filename = filter_filename(dirname, hash_str)\n",
    "        assert len(filename) == 1\n",
    "        data_record = pload(dirname + filename[0])\n",
    "        model = load_model(data_record[\"best_model_dict\"], device=device)\n",
    "        model.eval() \n",
    "\n",
    "        t_list = []\n",
    "        for i in range(100):\n",
    "            t_start = time.time()\n",
    "            preds, info = model(\n",
    "                data,\n",
    "                pred_steps=np.arange(1,max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "                latent_pred_steps=None,\n",
    "                is_recons=False,\n",
    "                use_grads=False,\n",
    "                is_y_diff=False,\n",
    "                is_rollout=False,\n",
    "            )\n",
    "            t_end = time.time()\n",
    "            t_list.append(t_end - t_start)\n",
    "            del preds\n",
    "            gc.collect()\n",
    "        full_time = np.mean(t_list)\n",
    "        n_params = get_num_params(model)\n",
    "\n",
    "        if model.__class__.__name__ == \"Contrastive\":\n",
    "            t_list_evo = []\n",
    "            for i in range(100):\n",
    "                t_start = time.time()\n",
    "                preds, info = model(\n",
    "                    data,\n",
    "                    pred_steps=[],\n",
    "                    latent_pred_steps=np.arange(1,max(parse_multi_step(args_test.multi_step).keys())+1),\n",
    "                    is_recons=False,\n",
    "                    use_grads=False,\n",
    "                    is_y_diff=False,\n",
    "                    is_rollout=False,\n",
    "                )\n",
    "                t_end = time.time()\n",
    "                t_list_evo.append(t_end - t_start)\n",
    "                del preds\n",
    "                gc.collect()\n",
    "            evo_time = np.mean(t_list_evo)\n",
    "            n_params_evo = get_num_params(model.evolution_op)\n",
    "            print(\"hash {}, full time: {:.6f} +- {:.6f}  evo time: {:.6f} +- {:.6f}. #params: {}  #params_evo: {}.\".format(\n",
    "                hash_str, full_time, np.std(t_list),\n",
    "                evo_time, np.std(t_list_evo),\n",
    "                n_params, n_params_evo,\n",
    "            ))\n",
    "            all_list_dict[hash_str] = {\n",
    "                \"evo\": t_list_evo,\n",
    "                \"full\": t_list,\n",
    "                \"n_params\": n_params,\n",
    "                \"n_params_evo\": n_params_evo,\n",
    "            }\n",
    "        else:\n",
    "            print(\"hash {}, full time: {:.6f} +- {:.6f}. #params: {}\".format(\n",
    "                hash_str, full_time, np.std(t_list), n_params))\n",
    "            all_list_dict[hash_str] = {\n",
    "                \"full\": t_list,\n",
    "                \"n_params\": n_params,\n",
    "            }\n",
    "    pdump(all_list_dict, f\"all_dict_1d_timing{suffix}.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19ff9a-6a81-4549-9e20-4b34ece5526c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ablation latent size @ turing3:\n",
    "all_hash = [\n",
    "    \"ldDNKnog_turing2\", # 512\n",
    "    \"JLf4tEYC_turing2\", # 256\n",
    "    \"Un6ae7ja_turing2\", # 128\n",
    "    \"5TFpW2r7\", # 64\n",
    "    \"xuaWUuBJ\", # 32\n",
    "    \"WO0JMG5U\", # 16\n",
    "    \"9dhW7XBI_turing2\", # 8\n",
    "    \"aKXmat5Z_turing2\", # 4\n",
    "]\n",
    "get_timing_1d(all_hash, suffix=\"_ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599bfea-97bc-4b95-b132-b006e43cdb57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FNO @ turing3:\n",
    "all_hash = [\n",
    "    \"N+XmBTW+_turing3\",\n",
    "    \"OlaHQmVh_turing3\",\n",
    "    \"95ODTwFm_turing3\",\n",
    "    \"k6pUtyDT_turing3\",\n",
    "    \"tMtuBQbF_turing3\",\n",
    "    \"6AEMxxdG_turing3\",\n",
    "    \"94vcmAJH_turing3\",\n",
    "    \"ajb+NVzF_turing3\",\n",
    "    \"0di76Adz_turing3\",\n",
    "    \"F0ge5kGj_turing3\",\n",
    "    \"Jr2biWNP_turing3\",\n",
    "    \"kxU74861_turing3\",\n",
    "]\n",
    "get_timing_1d(all_hash, suffix=\"_fno\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

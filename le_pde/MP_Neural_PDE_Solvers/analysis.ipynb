{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd2631-4db2-48ea-8fd8-970a55e53a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import pdb\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..', '..'))\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..', '..', '..', '..'))\n",
    "from MP_Neural_PDE_Solvers.equations.PDEs import *\n",
    "from MP_Neural_PDE_Solvers.common.utils import HDF5Dataset, GraphCreator, p\n",
    "from MP_Neural_PDE_Solvers.experiments.models_gnn import MP_PDE_Solver\n",
    "from MP_Neural_PDE_Solvers.experiments.models_cnn import BaseCNN\n",
    "from MP_Neural_PDE_Solvers.experiments.models_fno import FNO1d\n",
    "from MP_Neural_PDE_Solvers.experiments.train_helper import *\n",
    "from MP_Neural_PDE_Solvers.experiments.train import test\n",
    "from lamp.pytorch_net.util import pload, get_machine_name, filter_filename, init_args, Interp1d_torch, to_np_array\n",
    "\n",
    "def check_directory() -> None:\n",
    "    \"\"\"\n",
    "    Check if log directory exists within experiments\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f'experiments/log'):\n",
    "        os.mkdir(f'experiments/log')\n",
    "    if not os.path.exists(f'models'):\n",
    "        os.mkdir(f'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68047937-12d2-4bd0-9f8a-af936127373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"models/\"\n",
    "# Choose one of the following lines to test the corresponding baseline:\n",
    "model_name, uniform_sample = \"BaseCNN\", 2\n",
    "model_name, uniform_sample = \"BaseCNN\", 4\n",
    "model_name, uniform_sample = \"BaseCNN\", -1\n",
    "model_name, uniform_sample = \"FNO\", 2\n",
    "model_name, uniform_sample = \"FNO\", 4\n",
    "model_name, uniform_sample = \"FNO\", -1\n",
    "model_name, uniform_sample = \"GNN\", 2\n",
    "model_name, uniform_sample = \"GNN\", 4\n",
    "model_name, uniform_sample = \"GNN\", -1\n",
    "\n",
    "# Other configurations:\n",
    "is_full_eval = True\n",
    "resolution = 100 // uniform_sample if uniform_sample != -1 and model_name == \"GNN\" else 100\n",
    "filename_core = f\"GNN_CE_E2_{model_name}_xresolution{resolution}-200_uni{uniform_sample}_n3_tw25_unrolling1_server\"\n",
    "# Find the saved model under \"lamp/MP_Neural_PDE_Solvers/models/\":\n",
    "filenames = filter_filename(\"models/\", include=[filename_core, \".pt\"])\n",
    "assert len(filenames) == 1, f\"Find {len(filenames)} files satisfying the condition: {filenames}. Choose one of them.\"\n",
    "filename = filenames[0]\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31cf14f-3d33-44f0-b814-b1d01c74f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = init_args({\n",
    "    \"experiment\": \"E2\",\n",
    "    \"model\": model_name,\n",
    "    \"base_resolution\": [250,100],\n",
    "    \"time_window\": 25,\n",
    "    \"uniform_sample\": uniform_sample,\n",
    "    \"batch_size\": 4,\n",
    "    \"neighbors\": 3,\n",
    "    \"parameter_ablation\": False,\n",
    "    \"nr_gt_steps\": 2,\n",
    "})\n",
    "if model_name == \"GNN\" and uniform_sample != -1:\n",
    "    args.base_resolution = [250, 100//uniform_sample]\n",
    "    if uniform_sample == 3:\n",
    "        args.base_resolution = [250, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929df5d-fd18-4ed8-a565-d82b4af16aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HDF5Dataset(\n",
    "    '../data/mppde1d_data/CE_test_E2.h5',\n",
    "    pde=\"CE\",\n",
    "    mode='test',\n",
    "    base_resolution=args.base_resolution,\n",
    "    super_resolution=[250, 200],\n",
    "    uniform_sample=uniform_sample,\n",
    "    is_return_super=True,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=args.batch_size,\n",
    "                         shuffle=False,\n",
    "                         num_workers=0)\n",
    "pde = CE(device=device)\n",
    "pde.tmin = 0.0\n",
    "pde.tmax = 4.0\n",
    "pde.grid_size = [250, 100]\n",
    "\n",
    "if model_name == \"GNN\" and uniform_sample != -1:\n",
    "    args.base_resolution = [250, 100//uniform_sample]\n",
    "    pde.grid_size = [250, 100//uniform_sample]\n",
    "    if uniform_sample == 3:\n",
    "        args.base_resolution = [250, 34]\n",
    "        pde.grid_size = [250, 34]\n",
    "\n",
    "\n",
    "eq_variables = {}\n",
    "if not args.parameter_ablation:\n",
    "    if args.experiment == 'E2':\n",
    "        print(f'Beta parameter added to the GNN solver')\n",
    "        eq_variables['beta'] = 0.2\n",
    "    elif args.experiment == 'E3':\n",
    "        print(f'Alpha, beta, and gamma parameter added to the GNN solver')\n",
    "        eq_variables['alpha'] = 3.\n",
    "        eq_variables['beta'] = 0.4\n",
    "        eq_variables['gamma'] = 1.\n",
    "    elif (args.experiment == 'WE3'):\n",
    "        print('Boundary parameters added to the GNN solver')\n",
    "        eq_variables['bc_left'] = 1\n",
    "        eq_variables['bc_right'] = 1\n",
    "\n",
    "graph_creator = GraphCreator(pde=pde,\n",
    "                             neighbors=args.neighbors,\n",
    "                             time_window=args.time_window,\n",
    "                             t_resolution=args.base_resolution[0],\n",
    "                             x_resolution=args.base_resolution[1]).to(device)\n",
    "\n",
    "if args.model == 'GNN':\n",
    "    model = MP_PDE_Solver(pde=pde,\n",
    "                          time_window=graph_creator.tw,\n",
    "                          eq_variables=eq_variables).to(device)\n",
    "elif args.model == 'BaseCNN':\n",
    "    model = BaseCNN(pde=pde,\n",
    "                    time_window=args.time_window).to(device)\n",
    "elif args.model == 'FNO':\n",
    "    modes = min(16, (100 // uniform_sample) // 2 + 1)\n",
    "    model = FNO1d(pde=pde,\n",
    "                  modes=modes, width=64, input_size=args.time_window, output_size=args.time_window).to(device)\n",
    "else:\n",
    "    raise Exception(\"Wrong model specified\")\n",
    "model.load_state_dict(torch.load(dirname + filename))\n",
    "model.to(device)\n",
    "steps = [t for t in range(graph_creator.tw, 250-graph_creator.tw + 1)]\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "if uniform_sample == -1:\n",
    "    is_full_eval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4936da-9d10-4b00-b415-bde097ca99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loss for full trajectory unrolling, we report this loss in the paper\n",
    "Args:\n",
    "    model (torch.nn.Module): neural network PDE solver\n",
    "    steps (list): input list of possible starting (time) points\n",
    "    nr_gt_steps (int): number of numerical input timesteps\n",
    "    nx_base_resolution (int): spatial resolution of numerical baseline\n",
    "    loader (DataLoader): dataloader [valid, test]\n",
    "    graph_creator (GraphCreator): helper object to handle graph data\n",
    "    criterion (torch.nn.modules.loss): criterion for training\n",
    "    device (torch.cuda.device): device (cpu/gpu)\n",
    "Returns:\n",
    "    torch.Tensor: valid/test losses\n",
    "\"\"\"\n",
    "\n",
    "########################################################################\n",
    "# Here we evaluated the loss on the full 100 nodes compared with ground-truth, \n",
    "# wheter the model was trained on the 25, 50 or 100-node dataset. In this way,\n",
    "# All models are compared on the same ground-truth.\n",
    "#########################################################################\n",
    "\n",
    "model = model\n",
    "steps = steps\n",
    "batch_size = args.batch_size\n",
    "nr_gt_steps = args.nr_gt_steps\n",
    "nx_base_resolution = args.base_resolution[1]\n",
    "loader = test_loader\n",
    "graph_creator = graph_creator\n",
    "criterion = criterion\n",
    "device = device\n",
    "\n",
    "losses = []\n",
    "losses_base = []\n",
    "for (u_base, u_super, u_ori, x, x_ori, variables) in loader:\n",
    "    losses_tmp = []\n",
    "    losses_base_tmp = []\n",
    "    with torch.no_grad():\n",
    "        same_steps = [graph_creator.tw * nr_gt_steps] * batch_size  # [50] * batch_size:16\n",
    "        data, labels = graph_creator.create_data(u_super, same_steps)  # first time: data: from 25:50, label: from 50:75\n",
    "        if f'{model}' == 'GNN':\n",
    "            graph = graph_creator.create_graph(data, labels, x, variables, same_steps, uniform_sample=uniform_sample).to(device)\n",
    "            pred = model(graph)\n",
    "            loss = criterion(pred, graph.y) / nx_base_resolution\n",
    "        else:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            pred = model(data)\n",
    "            loss = criterion(pred, labels) / nx_base_resolution\n",
    "\n",
    "        if is_full_eval:\n",
    "            fnc = Interp1d_torch()\n",
    "            x_expand = torch.repeat_interleave(x_ori, 25, dim=0).to(device)\n",
    "            if model_name == \"GNN\":\n",
    "                pred_permute = pred.reshape(batch_size, -1, pred.shape[-1]).permute(0,2,1)  # [B, time_step, n_nodes]\n",
    "                pred_permute = pred_permute.reshape(-1, pred_permute.shape[-1])  # [B*time_step, n_nodes]\n",
    "                pred_interp = fnc(x_expand[...,::uniform_sample], pred_permute.reshape(-1,pred_permute.shape[-1]), x_expand)\n",
    "                pred_interp = pred_interp.reshape(batch_size, -1, pred_interp.shape[-1])\n",
    "                labels_expand = u_ori[:,same_steps[0]:same_steps[0]+25].to(device)\n",
    "                loss = criterion(pred_interp, labels_expand) / 100\n",
    "            else:\n",
    "                pred_interp = fnc(x_expand[...,::uniform_sample], pred.reshape(-1,pred.shape[-1]), x_expand)\n",
    "                pred_interp = pred_interp.reshape(pred.shape[0], -1, pred_interp.shape[-1])\n",
    "                # labels_narrow = u_super[:,same_steps[0]:same_steps[0]+25].to(device)\n",
    "                labels_expand = u_ori[:,same_steps[0]:same_steps[0]+25].to(device)\n",
    "                loss = criterion(pred_interp, labels_expand) / 100\n",
    "\n",
    "        losses_tmp.append(loss / batch_size)\n",
    "\n",
    "        # Unroll trajectory and add losses which are obtained for each unrolling\n",
    "        # for step in list(range(50, 225, 25)):\n",
    "        for step in list(range(graph_creator.tw * (nr_gt_steps + 1), graph_creator.t_res - graph_creator.tw, graph_creator.tw)):\n",
    "            same_steps = [step] * batch_size\n",
    "            _, labels = graph_creator.create_data(u_super, same_steps)\n",
    "            if f'{model}' == 'GNN':\n",
    "                graph = graph_creator.create_next_graph(graph, pred, labels, same_steps).to(device)\n",
    "                pred = model(graph)\n",
    "                loss = criterion(pred, graph.y) / nx_base_resolution  # pred/graph.y: [B*n_nodes, time_steps:25]\n",
    "            else:\n",
    "                labels = labels.to(device)\n",
    "                pred = model(pred)\n",
    "                loss = criterion(pred, labels) / nx_base_resolution  # pred, labels: [B, time_steps, n_nodes]\n",
    "\n",
    "            if is_full_eval:\n",
    "                fnc = Interp1d_torch()\n",
    "                x_expand = torch.repeat_interleave(x_ori, 25, dim=0).to(device)\n",
    "                if model_name == \"GNN\":\n",
    "                    pred_permute = pred.reshape(batch_size, -1, pred.shape[-1]).permute(0,2,1)  # [B, time_step, n_nodes]\n",
    "                    pred_permute = pred_permute.reshape(-1, pred_permute.shape[-1])  # [B*time_step, n_nodes]\n",
    "                    pred_interp = fnc(x_expand[...,::uniform_sample], pred_permute.reshape(-1,pred_permute.shape[-1]), x_expand)\n",
    "                    pred_interp = pred_interp.reshape(batch_size, -1, pred_interp.shape[-1])\n",
    "                    labels_expand = u_ori[:,same_steps[0]:same_steps[0]+25].to(device)\n",
    "                    loss = criterion(pred_interp, labels_expand) / 100\n",
    "                else:\n",
    "                    pred_interp = fnc(x_expand[...,::uniform_sample], pred.reshape(-1,pred.shape[-1]), x_expand)\n",
    "                    pred_interp = pred_interp.reshape(pred.shape[0], -1, pred_interp.shape[-1])\n",
    "                    # labels_narrow = u_super[:,same_steps[0]:same_steps[0]+25].to(device)\n",
    "                    labels_expand = u_ori[:,same_steps[0]:same_steps[0]+25].to(device)\n",
    "                loss = criterion(pred_interp, labels_expand) / 100\n",
    "            losses_tmp.append(loss / batch_size)  # batch_size = 16\n",
    "\n",
    "        # # Losses for numerical baseline\n",
    "        # for step in range(graph_creator.tw * nr_gt_steps, graph_creator.t_res - graph_creator.tw + 1,\n",
    "        #                   graph_creator.tw):\n",
    "        #     same_steps = [step] * batch_size\n",
    "        #     _, labels_super = graph_creator.create_data(u_super, same_steps)\n",
    "        #     _, labels_base = graph_creator.create_data(u_base, same_steps)\n",
    "        #     pdb.set_trace()\n",
    "        #     loss_base = criterion(labels_super, labels_base) / nx_base_resolution\n",
    "        #     losses_base_tmp.append(loss_base / batch_size)\n",
    "\n",
    "    losses.append(torch.sum(torch.stack(losses_tmp)))\n",
    "    losses_base.append(torch.tensor(0., dtype=torch.float32))\n",
    "    # losses_base.append(torch.sum(torch.stack(losses_base_tmp)))\n",
    "\n",
    "losses = torch.stack(losses)\n",
    "losses_base = torch.stack(losses_base)\n",
    "print(f'Unrolled forward losses {torch.mean(losses)}')\n",
    "print(f'Unrolled forward base losses {torch.mean(losses_base)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
